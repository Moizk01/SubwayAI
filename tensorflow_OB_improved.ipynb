{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdZ5s6Ut_qPJ"
      },
      "outputs": [],
      "source": [
        "# This jupyter notebook was developed in Colab.\n",
        "# The train and test set were upload to my Drive and were fetched directly from Google Drive\n",
        "# Current Directory in uses:\n",
        "# - training_path = '/content/drive/My Drive/Colab Notebooks/5_shot/train'\n",
        "# - testing_path = '/content/drive/My Drive/Colab Notebooks/5_shot/test'\n",
        "\n",
        "\n",
        "\n",
        "# To run script:\n",
        "# 1. update the training_path and testing_path (line 17 and 18) to where the dataset is located\n",
        "# 2. Run all the cells sequencially\n",
        "\n",
        "\n",
        "# Set path to train/test dataset (i.e., images)\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "training_path = '/content/drive/My Drive/Colab Notebooks/5_shot/train'\n",
        "testing_path = '/content/drive/My Drive/Colab Notebooks/5_shot/test'"
      ],
      "id": "RdZ5s6Ut_qPJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a352ba3-b6c7-42d0-86e1-863118fa7560",
        "outputId": "4ea021c6-7513-49fb-8dcf-bf74f0678baf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 110 files belonging to 22 classes.\n",
            "Using 88 files for training.\n",
            "Found 110 files belonging to 22 classes.\n",
            "Using 22 files for validation.\n",
            "(32, 530, 1020, 3) (32,)\n"
          ]
        }
      ],
      "source": [
        "### Load Data and split training dataset into training and validation sets\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        " # Low batch size for faster processing\n",
        "batch_size = 32\n",
        "# Defined Image size\n",
        "img_height = 530\n",
        "img_width = 1020\n",
        "# Defined class (integer label type)\n",
        "class_names = list(range(0,22))\n",
        "for i in range(len(class_names)):\n",
        "    class_names[i] = str(class_names[i])\n",
        "label_type = 'int'\n",
        "\n",
        "# Split the trianning set into: 20% validation, 80% training\n",
        "# Define parameters for the sets\n",
        "training_set = tf.keras.utils.image_dataset_from_directory(\n",
        "    training_path,\n",
        "    subset=\"training\",\n",
        "    validation_split=0.2,\n",
        "    label_mode=label_type,\n",
        "    class_names=class_names,\n",
        "    seed=123, # Randomize the set to prevent overfitting by sequence\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size\n",
        "    )\n",
        "\n",
        "validation_set = tf.keras.utils.image_dataset_from_directory(\n",
        "    training_path,\n",
        "    subset=\"validation\",\n",
        "    validation_split=0.2,\n",
        "    label_mode=label_type,\n",
        "    class_names=class_names,\n",
        "    seed=123,  # Randomize the set to prevent overfitting by sequence\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size\n",
        "    )\n",
        "\n",
        "# Set autotune function to automatically tune parameters dynamically at runtime\n",
        "AT = tf.data.AUTOTUNE\n",
        "training_set = training_set.cache().shuffle(1000).prefetch(buffer_size=AT) # Shuffle to reduce the likelihood of overfitting\n",
        "validation_set = validation_set.cache().prefetch(buffer_size=AT)"
      ],
      "id": "2a352ba3-b6c7-42d0-86e1-863118fa7560"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0d676af-2f76-419c-856a-96fc62b5c487"
      },
      "outputs": [],
      "source": [
        "### Load base model to be trained\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "\n",
        "# Initiate a base model based on VGG16\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(530, 1020, 3))\n",
        "len(base_model.layers)\n",
        "\n",
        "# Build a training model\n",
        "model = tf.keras.models.Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(1024, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(22, activation='softmax')  # Assuming 22 classes\n",
        "])"
      ],
      "id": "d0d676af-2f76-419c-856a-96fc62b5c487"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yjFHrYsNful2"
      },
      "id": "yjFHrYsNful2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze the base model's pre-trained layers\n",
        "base_model.trainable = False\n",
        "\n",
        "# Re-compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
        "model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "epochs_initial = 10\n",
        "history_train = model.fit(\n",
        "    training_set,\n",
        "    validation_data=validation_set,\n",
        "    epochs=epochs_initial\n",
        "    callbacks=[early_stopping, model_checkpoint]\n",
        ")\n",
        "\n",
        "# # Fine-tuning\n",
        "# base_model.trainable = True\n",
        "# fine_tune_at = 9 * len(base_model.layers) // 10\n",
        "\n",
        "# for layer in base_model.layers[:fine_tune_at]:\n",
        "#     layer.trainable = False\n",
        "\n",
        "# # Use a smaller learning rate for fine-tuning\n",
        "# optimizer_fine = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
        "# model.compile(optimizer=optimizer_fine, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# # Fine-tune the model\n",
        "# epochs_fine_tune = 2\n",
        "# history_finetune = model.fit(\n",
        "#     training_set,\n",
        "#     validation_data=validation_set,\n",
        "#     epochs=epochs_fine_tune,\n",
        "#     callbacks=[early_stopping, model_checkpoint]\n",
        "# )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBsMFuffPb5p",
        "outputId": "1c7380cc-95fa-46b2-d17d-d15eb68224ae"
      },
      "id": "TBsMFuffPb5p",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "3/3 [==============================] - 513s 167s/step - loss: 48.7940 - accuracy: 0.0341 - val_loss: 18.4062 - val_accuracy: 0.0455\n",
            "Epoch 2/10\n",
            "3/3 [==============================] - 497s 164s/step - loss: 17.6468 - accuracy: 0.0341 - val_loss: 4.2802 - val_accuracy: 0.0909\n",
            "Epoch 3/10\n",
            "3/3 [==============================] - 493s 163s/step - loss: 3.9990 - accuracy: 0.0795 - val_loss: 3.2515 - val_accuracy: 0.0909\n",
            "Epoch 4/10\n",
            "3/3 [==============================] - 496s 185s/step - loss: 3.1367 - accuracy: 0.0455 - val_loss: 3.1810 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/10\n",
            "3/3 [==============================] - 494s 163s/step - loss: 3.0933 - accuracy: 0.0682 - val_loss: 3.1951 - val_accuracy: 0.0455\n",
            "Epoch 6/10\n",
            "3/3 [==============================] - 493s 163s/step - loss: 3.0815 - accuracy: 0.0568 - val_loss: 3.1310 - val_accuracy: 0.0455\n",
            "Epoch 7/10\n",
            "3/3 [==============================] - 494s 163s/step - loss: 3.1224 - accuracy: 0.0341 - val_loss: 3.1705 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/10\n",
            "3/3 [==============================] - 497s 185s/step - loss: 3.0870 - accuracy: 0.0568 - val_loss: 3.1734 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/10\n",
            "3/3 [==============================] - 496s 185s/step - loss: 3.1549 - accuracy: 0.0568 - val_loss: 3.1788 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/10\n",
            "3/3 [==============================] - 496s 184s/step - loss: 3.0759 - accuracy: 0.0682 - val_loss: 3.1158 - val_accuracy: 0.0000e+00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-SP6-Qc_oue"
      },
      "source": [],
      "id": "c-SP6-Qc_oue"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ba6f3f0-f687-48ab-8119-a2070a859fb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d84dc76-49e7-4fc2-8668-5d8f6acbf040"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 517 files belonging to 1 classes.\n",
            "17/17 [==============================] - 815s 48s/step\n"
          ]
        }
      ],
      "source": [
        "### Load test data and predict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    testing_path,\n",
        "    image_size=(530, 1020),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    label_mode=None  # Test Data isn't labelled\n",
        ")\n",
        "\n",
        "predictions = model.predict(test_ds)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Complete file paths\n",
        "test_filenames = test_ds.file_paths\n",
        "# Extract filenames without extension\n",
        "for i in range(len(test_filenames)):\n",
        "    path = test_filenames[i]\n",
        "    filename_without_extension = path.split('/')[-1].split('.')[0]\n",
        "    test_filenames[i] = filename_without_extension\n",
        "\n",
        "# Create a Excle Sheet for submission\n",
        "submission = pd.DataFrame({'ID': test_filenames, 'Category': predicted_classes})\n",
        "submission.to_csv('submission.csv', index=False)\n"
      ],
      "id": "1ba6f3f0-f687-48ab-8119-a2070a859fb7"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install packages\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "! pip install gurobipy\n",
        "from gurobipy import Model, GRB\n",
        "import gurobipy as gp\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Setup data\n",
        "np.random.seed(2344)\n",
        "N = np.random.randint(20, 26)\n",
        "\n",
        "A1 = np.array([np.random.permutation(N) for _ in range(N)])\n",
        "A2 = np.random.choice([0, 1], size=(N, N), p=[0.5, 0.5])\n",
        "\n",
        "df_A1 = pd.DataFrame(A1)\n",
        "print(df_A1)\n",
        "df_A2 = pd.DataFrame(A2)\n",
        "print(df_A2)\n",
        "\n",
        "# Part 1 1A\n",
        "model_min = Model()\n",
        "vars_min = model_min.addVars(N, N, vtype=GRB.BINARY)\n",
        "model_min.setObjective(sum(vars_min[i, j] * A1[i, j] for i in range(N) for j in range(N)), GRB.MINIMIZE)\n",
        "\n",
        "for i in range(N):\n",
        "  model_min.addConstr(sum(vars_min[i, j] for j in range(N)) == 1)\n",
        "  model_min.addConstr(sum(vars_min[j, i] for j in range(N)) == 1)\n",
        "\n",
        "model_min.optimize()\n",
        "\n",
        "solution = np.zeros((N, N), dtype=int)\n",
        "for i in range(N):\n",
        "  for j in range(N):\n",
        "    if vars_min[i, j].X > 0.5:\n",
        "      solution[i, j] = 1\n",
        "\n",
        "G = nx.from_numpy_array(solution, create_using=nx.DiGraph)\n",
        "pos = nx.spring_layout(G)\n",
        "nx.draw(G, pos, with_labels=True, node_color='skyblue', node_size=700, edge_color='k', linewidths=1, font_size=15, arrows=True)\n",
        "plt.show()\n",
        "\n",
        "# Part 1 1B\n",
        "model_max = Model()\n",
        "vars_max = model_max.addVars(N, N, vtype=GRB.BINARY)\n",
        "model_max.setObjective(sum(vars_max[i, j] * A1[i, j] for i in range(N) for j in range(N)), GRB.MAXIMIZE)\n",
        "\n",
        "for i in range(N):\n",
        "  model_max.addConstr(sum(vars_max[i, j] for j in range(N)) == 1)\n",
        "  model_max.addConstr(sum(vars_max[j, i] for j in range(N)) == 1)\n",
        "\n",
        "model_max.optimize()\n",
        "\n",
        "solution = np.zeros((N, N), dtype=int)\n",
        "for i in range(N):\n",
        "  for j in range(N):\n",
        "    if vars_max[i, j].X > 0.5:\n",
        "      solution[i, j] = int(vars_max[i, j].X if model_max.status == GRB.OPTIMAL else 0)\n",
        "\n",
        "G = nx.from_numpy_array(solution, create_using=nx.DiGraph)\n",
        "pos = nx.spring_layout(G)\n",
        "nx.draw(G, pos, with_labels=True, node_color='skyblue', node_size=700, edge_color='k', linewidths=1, font_size=15, arrows=True)\n",
        "plt.show()\n",
        "\n",
        "# Part 1 2\n",
        "model_2_way = Model()\n",
        "vars_2_way = model_2_way.addVars(N, N, vtype=GRB.BINARY)\n",
        "model_2_way.setObjective(gp.quicksum(vars_2_way[i, j] * A2[i, j] * A2[j, i] for i in range(N) for j in range(i+1, N)), GRB.MAXIMIZE)\n",
        "\n",
        "for i in range(N):\n",
        "  model_2_way.addConstr(gp.quicksum(vars_2_way[i, j] + vars_2_way[j, i] for j in range(N) if j != i) <= 1)\n",
        "\n",
        "model_2_way.optimize()\n",
        "\n",
        "solution = np.zeros((N, N), dtype=int)\n",
        "for i in range(N):\n",
        "  for j in range(N):\n",
        "    if vars_2_way[i, j].X > 0.5 and A2[i, j] == 1 and A2[j, i] == 1:\n",
        "      solution[i, j] = solution[j, i] = 1\n",
        "\n",
        "G = nx.from_numpy_array(solution, create_using=nx.DiGraph)\n",
        "pos = nx.spring_layout(G)\n",
        "nx.draw(G, pos, with_labels=True, node_color='skyblue', node_size=700, edge_color='k', linewidths=1, font_size=15, arrows=True)\n",
        "plt.show()\n",
        "\n",
        "# Part 2\n",
        "A1_compatible = np.where(A1 < 11, 1, 0)\n",
        "df_A1_compatible = pd.DataFrame(A1_compatible)\n",
        "print(df_A1_compatible)\n",
        "model_priorities = Model()\n",
        "vars_priorities = model_priorities.addVars(N, N, vtype=GRB.BINARY)\n",
        "model_priorities.setObjective(gp.quicksum(vars_priorities[i, j] for i in range(N) for j in range(N)), GRB.MAXIMIZE)\n",
        "\n",
        "for i in range(N):\n",
        "  model_priorities.addConstr(gp.quicksum(vars_priorities[i, j] for j in range(N)) <= 1)\n",
        "  model_priorities.addConstr(gp.quicksum(vars_priorities[j, i] for j in range(N)) <= 1)\n",
        "\n",
        "for i in range(N):\n",
        "    for j in range(N):\n",
        "        model_priorities.addConstr(vars_priorities[i, j] <= A1_compatible[i, j])\n",
        "        model_priorities.addConstr(vars_priorities[i, j] == vars_priorities[j, i])\n",
        "\n",
        "\n",
        "model_priorities.optimize()\n",
        "\n",
        "solution = np.zeros((N, N), dtype=int)\n",
        "for i in range(N):\n",
        "  for j in range(N):\n",
        "    if vars_priorities[i, j].X > 0.5:  # If the exchange is selected in the solution\n",
        "      solution[i, j] = 1\n",
        "\n",
        "G = nx.from_numpy_array(solution, create_using=nx.DiGraph)\n",
        "pos = nx.spring_layout(G)\n",
        "nx.draw(G, pos, with_labels=True, node_color='skyblue', node_size=700, edge_color='k', linewidths=1, font_size=15, arrows=True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "41Fgi6oPfvu9"
      },
      "id": "41Fgi6oPfvu9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UrSBLPNVo2Z3"
      },
      "id": "UrSBLPNVo2Z3",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}